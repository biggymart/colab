{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_TrainingAFeedForwardNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBAmgKm6wD5eM6+mS5/ed9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biggymart/colab/blob/main/02_TrainingAFeedForwardNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXmHak1QmRvF"
      },
      "source": [
        "PyTorch Tutorial\n",
        "\n",
        "https://www.youtube.com/watch?v=4p0G6tgNLis&t=28s\n",
        "\n",
        "First ever Neuron Network!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50hrlcZomhaP",
        "outputId": "fee65514-40b8-4c9b-ab43-6f5c6ea5b326"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# 1 - download dataset\n",
        "# 2 - create data loader\n",
        "# 3 - build model\n",
        "# 4 - train\n",
        "# 5 - save trained model\n",
        "\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "class FeedForwardNet(nn.Module):\n",
        "\n",
        "  def __init__(self): # constructor\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.dense_layers = nn.Sequential( # packing many layers into one block\n",
        "        nn.Linear(28*28, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 10)\n",
        "    )\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, input_data): # data flow\n",
        "    flattened_data = self.flatten(input_data)\n",
        "    logits = self.dense_layers(flattened_data)\n",
        "    predictions = self.softmax(logits)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def download_mnist_datasets():\n",
        "  train_data = datasets.MNIST(\n",
        "      root=\"data\", # New folder called \"data\"\n",
        "      download=True, # download plz if not already\n",
        "      train=True, # it is a train dataset\n",
        "      transform=ToTensor() # takes image in and reshapes it as tensor [0, 1]\n",
        "  )\n",
        "  validation_data = datasets.MNIST(\n",
        "      root=\"data\",\n",
        "      download=True,\n",
        "      train=False, # it is a non-train dataset\n",
        "      transform=ToTensor()\n",
        "  )\n",
        "  return train_data, validation_data\n",
        "\n",
        "\n",
        "def train_one_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "  for inputs, targets in data_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # calculate loss\n",
        "    predictions = model(inputs)\n",
        "    loss = loss_fn(predictions, targets)\n",
        "\n",
        "    # backpropagate loss and update weights\n",
        "    optimizer.zero_grad() # reset gradients\n",
        "    loss.backward() # backpropagation\n",
        "    optimizer.step() # update weight\n",
        "\n",
        "  print(f\"[INFO] Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimizer, device, epochs):\n",
        "  for i in range(epochs):\n",
        "    print(f\"[INFO] Epoch {i+1}\")\n",
        "    train_one_epoch(model, data_loader, loss_fn, optimizer, device)\n",
        "    print(\"========================\")\n",
        "  print(\"[INFO] Training is done\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # download MNIST dataset\n",
        "  train_data, _ = download_mnist_datasets()\n",
        "  print(\"[INFO] MNIST dataset downloaded\")\n",
        "\n",
        "  # create a data loader for the train set\n",
        "  train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "  # build model\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  else:\n",
        "    device = \"cpu\"\n",
        "  print(f\"[INFO] Using {device} device\")\n",
        "  feed_forward_net = FeedForwardNet().to(device) # [1] cuda [2] CPU\n",
        "\n",
        "  # instantiate loss function + optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(feed_forward_net.parameters(),\n",
        "                               lr=LEARNING_RATE)\n",
        "\n",
        "  # train model\n",
        "  train(feed_forward_net, train_data_loader, loss_fn, optimizer, device, EPOCHS)\n",
        "\n",
        "  # save model\n",
        "  torch.save(feed_forward_net.state_dict(), \"feedforwardnet.pth\")\n",
        "  print(\"[INFO] Model trained and stored at feedforwardnet.pth\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] MNIST dataset downloaded\n",
            "[INFO] Using cpu device\n",
            "[INFO] Epoch 1\n",
            "[INFO] Loss: 1.5145989656448364\n",
            "========================\n",
            "[INFO] Epoch 2\n",
            "[INFO] Loss: 1.5001498460769653\n",
            "========================\n",
            "[INFO] Epoch 3\n",
            "[INFO] Loss: 1.4983819723129272\n",
            "========================\n",
            "[INFO] Epoch 4\n",
            "[INFO] Loss: 1.4825791120529175\n",
            "========================\n",
            "[INFO] Epoch 5\n",
            "[INFO] Loss: 1.476203441619873\n",
            "========================\n",
            "[INFO] Epoch 6\n",
            "[INFO] Loss: 1.473862648010254\n",
            "========================\n",
            "[INFO] Epoch 7\n",
            "[INFO] Loss: 1.4728718996047974\n",
            "========================\n",
            "[INFO] Epoch 8\n",
            "[INFO] Loss: 1.4731394052505493\n",
            "========================\n",
            "[INFO] Epoch 9\n",
            "[INFO] Loss: 1.4724836349487305\n",
            "========================\n",
            "[INFO] Epoch 10\n",
            "[INFO] Loss: 1.4737366437911987\n",
            "========================\n",
            "[INFO] Training is done\n",
            "[INFO] Model trained and stored at feedforwardnet.pth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}